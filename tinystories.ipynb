{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TinyStories Language Model Experiments\n",
    "\n",
    "This notebook compares two different approaches for language modeling on the TinyStories dataset:\n",
    "1. A Deep Reservoir Computing-based Model\n",
    "2. A Transformer-based Model\n",
    "\n",
    "Both models will be evaluated on the same dataset with the same evaluation metrics for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Shared Configuration for Both Models\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "SHARED_CONFIG = {\n",
    "    # Data parameters\n",
    "    'MAX_STORIES': 100,                # Number of stories to use for training\n",
    "    \n",
    "    # Training parameters\n",
    "    'EPOCHS': 1,                       # Number of training epochs\n",
    "    'BATCH_SIZE': 32,                  # Batch size for training\n",
    "    'BLOCK_SIZE': 128,                 # Context size for training\n",
    "    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Evaluation parameters\n",
    "    'EVAL_INTERVAL': 200,              # Steps between evaluations\n",
    "    'EVAL_ITER': 50,                   # Number of batches for evaluation\n",
    "    'MAX_OUT_TOKENS': 100,             # Number of tokens to generate for samples\n",
    "    \n",
    "    # File paths\n",
    "    'RESERVOIR_SAVE_PATH': 'models/deep_reservoir_trained.pt',\n",
    "    'TRANSFORMER_SAVE_PATH': 'models/tiny_lm_trained.pt'\n",
    "}\n",
    "\n",
    "print(f\"Using device: {SHARED_CONFIG['DEVICE']}\")\n",
    "os.makedirs(\"models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install reservoirpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Import Common Libraries\n",
    "# -----------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2TokenizerFast\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import reservoirpy as rpy\n",
    "from reservoirpy.nodes import Reservoir\n",
    "\n",
    "# Reduce verbosity for ReservoirPy\n",
    "rpy.verbosity(0)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Data Download and Tokenization\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "def download_and_save_dataset(max_stories=None):\n",
    "    \"\"\"Downloads the TinyStories dataset and saves a subset if specified.\"\"\"\n",
    "    data_dir = \"data\"\n",
    "    if max_stories:\n",
    "        train_path = os.path.join(data_dir, f\"TinyStories-train-{max_stories}.txt\")\n",
    "        valid_path = os.path.join(data_dir, f\"TinyStories-valid-{max_stories}.txt\")\n",
    "    else:\n",
    "        train_path = os.path.join(data_dir, \"TinyStories-train.txt\")\n",
    "        valid_path = os.path.join(data_dir, \"TinyStories-valid.txt\")\n",
    "\n",
    "    if os.path.exists(train_path) and os.path.exists(valid_path):\n",
    "        print(f\"Dataset files already exist: {train_path}, {valid_path}\")\n",
    "        return train_path, valid_path\n",
    "\n",
    "    print(\"Downloading TinyStories dataset from Hugging Face...\")\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    ds = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "    print(f\"Saving training split to {train_path}...\")\n",
    "    with open(train_path, 'w', encoding='utf-8') as f:\n",
    "        for i, story in enumerate(tqdm(ds['train'])):\n",
    "            if max_stories and i >= max_stories:\n",
    "                break\n",
    "            f.write(story['text'] + '\\n')\n",
    "\n",
    "    print(f\"Saving validation split to {valid_path}...\")\n",
    "    with open(valid_path, 'w', encoding='utf-8') as f:\n",
    "        val_stories_to_save = max_stories // 10 if max_stories else None\n",
    "        for i, story in enumerate(tqdm(ds['validation'])):\n",
    "            if val_stories_to_save and i >= val_stories_to_save:\n",
    "                break\n",
    "            f.write(story['text'] + '\\n')\n",
    "    return train_path, valid_path\n",
    "\n",
    "def pre_tokenize_dataset(path, save_path):\n",
    "    print(f\"Running tokenization for {path}...\")\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        tokens = tokenizer.encode(text)\n",
    "        np.save(save_path, np.array(tokens, dtype=np.int32))\n",
    "        print(f\"Saved tokenized file to binary {save_path}\")\n",
    "\n",
    "class TinyStoriesDataset(data.Dataset):\n",
    "    def __init__(self, tokenized_path, block_size: int):\n",
    "        self.block_size = block_size\n",
    "        self.data = np.load(tokenized_path, mmap_mode='r')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        source = torch.from_numpy(chunk[:-1].astype(np.int64))\n",
    "        target = torch.from_numpy(chunk[1:].astype(np.int64))\n",
    "        return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Download and Prepare Dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Download dataset\n",
    "train_txt_path, val_txt_path = download_and_save_dataset(max_stories=SHARED_CONFIG[\"MAX_STORIES\"])\n",
    "\n",
    "# Tokenize training data\n",
    "train_tokenized_path = train_txt_path.replace('.txt', '.npy')\n",
    "if not os.path.exists(train_tokenized_path):\n",
    "    pre_tokenize_dataset(train_txt_path, train_tokenized_path)\n",
    "\n",
    "# Tokenize validation data\n",
    "val_tokenized_path = val_txt_path.replace('.txt', '.npy')\n",
    "if not os.path.exists(val_tokenized_path):\n",
    "    pre_tokenize_dataset(val_txt_path, val_tokenized_path)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TinyStoriesDataset(train_tokenized_path, SHARED_CONFIG['BLOCK_SIZE'])\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=SHARED_CONFIG['BATCH_SIZE'], shuffle=True)\n",
    "\n",
    "val_dataset = TinyStoriesDataset(val_tokenized_path, SHARED_CONFIG['BLOCK_SIZE'])\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=SHARED_CONFIG['BATCH_SIZE'])\n",
    "\n",
    "print(f\"Data preparation complete! Tokenizer vocabulary size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T18:12:30.951097Z",
     "iopub.status.busy": "2025-07-26T18:12:30.950456Z",
     "iopub.status.idle": "2025-07-26T18:14:18.920115Z",
     "shell.execute_reply": "2025-07-26T18:14:18.918978Z",
     "shell.execute_reply.started": "2025-07-26T18:12:30.951072Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Deep Reservoir Model Definition & Training\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class ReservoirBlock(nn.Module):\n",
    "    \"\"\"A single block containing parallel reservoirs and a readout.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.reservoirs = []\n",
    "\n",
    "        for res_config in config['reservoirs_per_block']:\n",
    "            self.reservoirs.append(\n",
    "                Reservoir(\n",
    "                    units=res_config['reservoir_size'],\n",
    "                    sr=res_config['spectral_radius'],\n",
    "                    lr=res_config['leaking_rate']\n",
    "                )\n",
    "            )\n",
    "\n",
    "        total_reservoir_size = sum(res['reservoir_size'] for res in config['reservoirs_per_block'])\n",
    "        self.readout = nn.Sequential(\n",
    "            nn.Linear(total_reservoir_size, config['readout_hidden_size']),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(config['readout_hidden_size'], config['embedding_dim'])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        batch_size, _, _ = x.shape\n",
    "        all_res_states = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            sequence_np = x[i].detach().cpu().numpy()\n",
    "            states_for_sequence = []\n",
    "            for reservoir in self.reservoirs:\n",
    "                states = reservoir.run(sequence_np, reset=True)\n",
    "                states_for_sequence.append(torch.from_numpy(states).float())\n",
    "            combined_states = torch.cat(states_for_sequence, dim=1)\n",
    "            all_res_states.append(combined_states)\n",
    "\n",
    "        batch_states = torch.stack(all_res_states).to(device)\n",
    "        update_vector = self.readout(batch_states)\n",
    "        return update_vector\n",
    "\n",
    "class DeepReservoirModel(nn.Module):\n",
    "    \"\"\"A deep model composed of stacked ReservoirBlocks.\"\"\"\n",
    "    def __init__(self, vocab_size, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(vocab_size, config['embedding_dim'])\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ReservoirBlock(config) for _ in range(config['num_blocks'])\n",
    "        ])\n",
    "        self.final_head = nn.Linear(config['embedding_dim'], vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        x = self.embedding(idx)\n",
    "        for block in self.blocks:\n",
    "            update = block(x)\n",
    "            x = x + update # Residual connection\n",
    "        logits = self.final_head(x)\n",
    "        return logits\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_reservoir_model(model, data_loader, config):\n",
    "    \"\"\"Evaluates the reservoir model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    num_batches = 0\n",
    "    for x, y in data_loader:\n",
    "        if num_batches >= config['EVAL_ITER']:\n",
    "            break\n",
    "        x, y = x.to(config['DEVICE']), y.to(config['DEVICE'])\n",
    "        logits = model(x)\n",
    "        B, T, C = logits.shape\n",
    "        loss = criterion(logits.view(B * T, C), y.view(B * T))\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    model.train()\n",
    "    return total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_from_reservoir(model, context_str, max_new_tokens, config):\n",
    "    \"\"\"Generates text from the reservoir model.\"\"\"\n",
    "    model.eval()\n",
    "    start_indices = tokenizer.encode(context_str)\n",
    "    context = torch.tensor(start_indices, dtype=torch.long, device=config['DEVICE']).unsqueeze(0)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        current_context = context[:, -config['BLOCK_SIZE']:]\n",
    "        logits = model(current_context)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        context = torch.cat((context, idx_next), dim=1)\n",
    "\n",
    "    model.train()\n",
    "    return tokenizer.decode(context.squeeze().tolist())\n",
    "\n",
    "# Define Reservoir model specific configuration\n",
    "reservoir_config = {\n",
    "    # Model architecture\n",
    "    'embedding_dim': 256,\n",
    "    'num_blocks': 4,\n",
    "    'reservoirs_per_block': [\n",
    "        {'reservoir_size': 128, 'leaking_rate': 0.3, 'spectral_radius': 0.9},\n",
    "        {'reservoir_size': 256, 'leaking_rate': 0.1, 'spectral_radius': 0.9},\n",
    "    ],\n",
    "    'readout_hidden_size': 128,\n",
    "    \n",
    "    # Training params from shared config\n",
    "    'BATCH_SIZE': SHARED_CONFIG['BATCH_SIZE'],\n",
    "    'BLOCK_SIZE': SHARED_CONFIG['BLOCK_SIZE'],\n",
    "    'EVAL_INTERVAL': SHARED_CONFIG['EVAL_INTERVAL'],\n",
    "    'EVAL_ITER': SHARED_CONFIG['EVAL_ITER'],\n",
    "    'DEVICE': SHARED_CONFIG['DEVICE'],\n",
    "    'SAVE_PATH': SHARED_CONFIG['RESERVOIR_SAVE_PATH'],\n",
    "    'LR': 0.001,\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "reservoir_model = DeepReservoirModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    config=reservoir_config\n",
    ").to(SHARED_CONFIG['DEVICE'])\n",
    "\n",
    "print(f\"Reservoir Model initialized on {SHARED_CONFIG['DEVICE']} with {sum(p.numel() for p in reservoir_model.parameters() if p.requires_grad):,} trainable parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Train the Reservoir Model\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(reservoir_model.parameters(), lr=reservoir_config['LR'])\n",
    "\n",
    "train_losses = []\n",
    "val_perplexities = []\n",
    "steps = []\n",
    "total_batches = 0\n",
    "\n",
    "print(\"Starting Reservoir Model training...\")\n",
    "for epoch in range(1, SHARED_CONFIG['EPOCHS'] + 1):\n",
    "    print(f\"--- Epoch {epoch}/{SHARED_CONFIG['EPOCHS']} ---\")\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    for x, y in pbar:\n",
    "        total_batches += 1\n",
    "        x, y = x.to(SHARED_CONFIG['DEVICE']), y.to(SHARED_CONFIG['DEVICE'])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = reservoir_model(x)\n",
    "        B, T, C = logits.shape\n",
    "        loss = criterion(logits.view(B * T, C), y.view(B * T))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        if total_batches > 0 and total_batches % SHARED_CONFIG['EVAL_INTERVAL'] == 0:\n",
    "            val_loss = eval_reservoir_model(reservoir_model, val_loader, reservoir_config)\n",
    "            perplexity = np.exp(val_loss)\n",
    "            val_perplexities.append(perplexity)\n",
    "            steps.append(total_batches)\n",
    "            print(\"\\n\" + \"-\" * 50)\n",
    "            print(f\"Validation Loss: {val_loss:.4f}, Validation Perplexity: {perplexity:.4f}\")\n",
    "            generated_text = generate_from_reservoir(\n",
    "                reservoir_model, \n",
    "                \"Once upon a time\", \n",
    "                SHARED_CONFIG['MAX_OUT_TOKENS'], \n",
    "                reservoir_config\n",
    "            )\n",
    "            print(\"--- Sample Generation ---\")\n",
    "            print(generated_text)\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "print(\"Reservoir Model training finished.\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('Reservoir Model - Training Loss')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(steps, val_perplexities, marker='o')\n",
    "plt.title('Reservoir Model - Validation Perplexity')\n",
    "plt.xlabel('Total Batches')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saving Reservoir model to {reservoir_config['SAVE_PATH']}...\")\n",
    "torch.save(reservoir_model.state_dict(), reservoir_config['SAVE_PATH'])\n",
    "print(\"Reservoir Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-26T18:14:30.703113Z",
     "iopub.status.busy": "2025-07-26T18:14:30.702804Z",
     "iopub.status.idle": "2025-07-26T18:15:31.420731Z",
     "shell.execute_reply": "2025-07-26T18:15:31.419588Z",
     "shell.execute_reply.started": "2025-07-26T18:14:30.703080Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Transformer Model Definition\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class TinyLM(nn.Module):\n",
    "    \"\"\"\n",
    "    A small language model based on the Transformer architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int = 50257, emb_dim: int = 768, block_size: int = 256, n_att_heads: int = 12,\n",
    "                 n_decoders: int = 12, device: str = 'cuda'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.block_size = block_size\n",
    "        self.token_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.pos_emb = nn.Embedding(block_size, emb_dim)\n",
    "        self.decoders = nn.Sequential(*(TransformerDecoder(emb_dim, block_size, n_att_heads)\n",
    "                                        for _ in range(n_decoders)))\n",
    "        self.final_linear = nn.Linear(emb_dim, vocab_size)\n",
    "        self.layer_norm = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def generate(self, context: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
    "        for _ in range(max_new_tokens):\n",
    "            context = context[:, -self.block_size:]\n",
    "            logits = self(context)[:, -1, :]\n",
    "            probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            context = torch.cat([context, next_token], dim=1)\n",
    "        return context\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_emb = self.token_emb(x)\n",
    "        pos_emb = self.pos_emb(torch.arange(min(self.block_size, x.size(1)), device=self.device))\n",
    "        x = token_emb + pos_emb\n",
    "        x = self.decoders(x)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.final_linear(x)\n",
    "        return logits\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer decoder block.\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim: int = 768, block_size: int = 256, n_heads: int = 12, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_projection = nn.Linear(emb_dim, 3 * emb_dim, bias=False)\n",
    "        self.register_buffer('tril', ~torch.tril(torch.ones(block_size, block_size)).type(torch.bool))\n",
    "        self.self_attention = nn.MultiheadAttention(emb_dim, n_heads, batch_first=True, dropout=0.2)\n",
    "        self.feed_fwd = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim * 4, emb_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.ln_1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln_2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln_1(x)\n",
    "        x_proj = self.head_projection(x)\n",
    "        q, k, v = x_proj.split(self.emb_dim, dim=-1)\n",
    "        x = x + self.self_attention(q, k, v, attn_mask=self.tril[:x.size(1), :x.size(1)], need_weights=False)[0]\n",
    "        x = x + self.feed_fwd(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_transformer_model(training_model: torch.nn.Module, val_loader: torch.utils.data.DataLoader, config: dict):\n",
    "    training_model.eval()\n",
    "    losses = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for k, (s_val, t_val) in enumerate(val_loader):\n",
    "        if k >= config['EVAL_ITER']:\n",
    "            break\n",
    "        s_val, t_val = s_val.to(config['DEVICE']), t_val.to(config['DEVICE'])\n",
    "        val_logits = training_model(s_val)\n",
    "        B, T, C = val_logits.shape\n",
    "        val_loss = criterion(val_logits.view(B * T, C), t_val.view(B * T))\n",
    "        losses.append(val_loss.item())\n",
    "    training_model.train()\n",
    "    return np.mean(losses)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_from_transformer(training_model: TinyLM, config: dict, prompt=\"Once upon a time\") -> str:\n",
    "    training_model.eval()\n",
    "    if prompt:\n",
    "        context = torch.tensor(tokenizer.encode(prompt), dtype=torch.long, device=config['DEVICE']).unsqueeze(0)\n",
    "    else:\n",
    "        context = torch.zeros((1, 1), dtype=torch.long, device=config['DEVICE'])\n",
    "    out_tokens = training_model.generate(context, max_new_tokens=config['MAX_OUT_TOKENS'])\n",
    "    training_model.train()\n",
    "    return tokenizer.decode(out_tokens[0].tolist())\n",
    "\n",
    "# Define Transformer model specific configuration\n",
    "transformer_config = {\n",
    "    # Model architecture\n",
    "    'EMB_SIZE': 384,\n",
    "    'N_ATTENTION_HEADS': 6,\n",
    "    'N_DECODER_BLOCKS': 6,\n",
    "    'VOCAB_SIZE': tokenizer.vocab_size,\n",
    "    \n",
    "    # Training params from shared config\n",
    "    'MAX_OUT_TOKENS': SHARED_CONFIG['MAX_OUT_TOKENS'],\n",
    "    'EVAL_INTERVAL': SHARED_CONFIG['EVAL_INTERVAL'],\n",
    "    'EVAL_ITER': SHARED_CONFIG['EVAL_ITER'],\n",
    "    'LR': 3e-4,\n",
    "    'BATCH_SIZE': SHARED_CONFIG['BATCH_SIZE'],\n",
    "    'BLOCK_SIZE': SHARED_CONFIG['BLOCK_SIZE'],\n",
    "    'DEVICE': SHARED_CONFIG['DEVICE'],\n",
    "    'SAVE_PATH': SHARED_CONFIG['TRANSFORMER_SAVE_PATH'],\n",
    "}\n",
    "assert transformer_config['EMB_SIZE'] % transformer_config['N_ATTENTION_HEADS'] == 0, \"Embedding size must be divisible by number of attention heads\"\n",
    "\n",
    "# Initialize the model\n",
    "transformer_model = TinyLM(\n",
    "    emb_dim=transformer_config['EMB_SIZE'],\n",
    "    block_size=transformer_config['BLOCK_SIZE'],\n",
    "    n_att_heads=transformer_config['N_ATTENTION_HEADS'],\n",
    "    n_decoders=transformer_config['N_DECODER_BLOCKS'],\n",
    "    vocab_size=transformer_config['VOCAB_SIZE'],\n",
    "    device=transformer_config['DEVICE']\n",
    ").to(transformer_config['DEVICE'])\n",
    "\n",
    "print(f\"Transformer Model initialized on {SHARED_CONFIG['DEVICE']} with {sum(p.numel() for p in transformer_model.parameters() if p.requires_grad):,} trainable parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Train the Transformer Model\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=transformer_config['LR'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Lists to store metrics for plotting\n",
    "transformer_train_losses = []\n",
    "transformer_val_perplexities = []\n",
    "transformer_steps = []\n",
    "total_batches = 0\n",
    "\n",
    "print(\"Starting Transformer Model training...\")\n",
    "for epoch in range(1, SHARED_CONFIG['EPOCHS'] + 1):\n",
    "    print(f\"--- Epoch {epoch}/{SHARED_CONFIG['EPOCHS']} ---\")\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    for batch in pbar:\n",
    "        sources, targets = batch\n",
    "        total_batches += 1\n",
    "        \n",
    "        sources, targets = sources.to(SHARED_CONFIG['DEVICE']), targets.to(SHARED_CONFIG['DEVICE'])\n",
    "        logits = transformer_model(sources)\n",
    "        \n",
    "        B, T, C = logits.shape\n",
    "        loss = criterion(logits.view(B * T, C), targets.view(B * T))\n",
    "        \n",
    "        transformer_train_losses.append(loss.item())\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if total_batches > 0 and total_batches % SHARED_CONFIG['EVAL_INTERVAL'] == 0:\n",
    "            val_loss = eval_transformer_model(transformer_model, val_loader, transformer_config)\n",
    "            perplexity = np.exp(val_loss)\n",
    "            transformer_val_perplexities.append(perplexity)\n",
    "            transformer_steps.append(total_batches)\n",
    "            print(\"\\n\" + \"-\" * 50)\n",
    "            print(f\"Validation Loss: {val_loss:.4f}, Validation Perplexity: {perplexity:.4f}\")\n",
    "            generated_text = generate_from_transformer(\n",
    "                transformer_model, \n",
    "                transformer_config, \n",
    "                prompt=\"Once upon a time\"\n",
    "            )\n",
    "            print(\"--- Sample Generation ---\")\n",
    "            print(generated_text)\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "print(\"Transformer Model training finished.\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(transformer_train_losses)\n",
    "plt.title('Transformer Model - Training Loss')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(transformer_steps, transformer_val_perplexities, marker='o')\n",
    "plt.title('Transformer Model - Validation Perplexity')\n",
    "plt.xlabel('Total Batches')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saving Transformer model to {transformer_config['SAVE_PATH']}...\")\n",
    "torch.save({\n",
    "    'model_state_dict': transformer_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'config': transformer_config,\n",
    "}, transformer_config['SAVE_PATH'])\n",
    "print(\"Transformer Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Compare Model Performance\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Visualize the validation perplexity of both models on the same plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, val_perplexities, marker='o', label='Reservoir Model')\n",
    "plt.plot(transformer_steps, transformer_val_perplexities, marker='s', label='Transformer Model')\n",
    "plt.title('Model Comparison - Validation Perplexity')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Perplexity (lower is better)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate some comparison text with the same prompt\n",
    "print(\"=\" * 80)\n",
    "print(\"Model Comparison - Text Generation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "prompt = \"Once upon a time, there was a little\"\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "\n",
    "# Generate from Reservoir model\n",
    "reservoir_text = generate_from_reservoir(\n",
    "    reservoir_model, \n",
    "    prompt, \n",
    "    SHARED_CONFIG['MAX_OUT_TOKENS'], \n",
    "    reservoir_config\n",
    ")\n",
    "print(\"Reservoir Model Output:\")\n",
    "print(\"-\" * 50)\n",
    "print(reservoir_text)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Generate from Transformer model\n",
    "transformer_text = generate_from_transformer(\n",
    "    transformer_model, \n",
    "    transformer_config, \n",
    "    prompt=prompt\n",
    ")\n",
    "print(\"Transformer Model Output:\")\n",
    "print(\"-\" * 50)\n",
    "print(transformer_text)\n",
    "\n",
    "# Compare model size\n",
    "reservoir_params = sum(p.numel() for p in reservoir_model.parameters() if p.requires_grad)\n",
    "transformer_params = sum(p.numel() for p in transformer_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Reservoir Model: {reservoir_params:,} parameters\")\n",
    "print(f\"Transformer Model: {transformer_params:,} parameters\")\n",
    "print(f\"Size difference: {abs(reservoir_params - transformer_params):,} parameters\")\n",
    "print(f\"The {'Reservoir' if reservoir_params < transformer_params else 'Transformer'} model is smaller by {abs(100 - (100 * min(reservoir_params, transformer_params) / max(reservoir_params, transformer_params))):.1f}%\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
